{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.datasets import STSDataset, SICKDataset, QuoraQuestionsDataset\n",
    "from modules.clean_text import tokenize_and_clean\n",
    "import logging\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Merge\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "LOG = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(asctime)s : %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "LOG.addHandler(handler)\n",
    "LOG.setLevel(logging.INFO)\n",
    "\n",
    "QUORA_FILE = 'C:\\\\dev_env\\\\ml\\\\datasets\\\\quora_questions_pair\\\\train.csv'\n",
    "STS_FILE = 'C:\\\\dev_env\\\\ml\\\\datasets\\\\sts\\\\sts_all.txt'\n",
    "SICK_FILE = 'C:\\\\dev_env\\ml\\\\datasets\\\\sick_2014\\\\SICK_complete.txt'\n",
    "WORD2VEC = 'C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\\\GoogleNews-vectors-negative300.bin'\n",
    "GLOVE = 'C:\\dev_env\\ml\\datasets\\glove.6B\\\\glove.6B.300d.gensim.txt'\n",
    "\n",
    "\n",
    "EMBEDDING_FILE = WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:07:35,155 : Vocabulary created. Size: 15230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:07:35,155 : Vocabulary created. Size: 15230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:07:35,155 : Vocabulary created. Size: 15230\n"
     ]
    }
   ],
   "source": [
    "train_df =  STSDataset(STS_FILE).data_frame()\n",
    "\n",
    "sentences_1 = []\n",
    "sentences_2 = []\n",
    "labels = []\n",
    "for index, row in train_df.iterrows():\n",
    "    sentences_1.append(tokenize_and_clean(row['s1']))\n",
    "    sentences_2.append(tokenize_and_clean(row['s2']))\n",
    "    labels.append(float(row['label']))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_1)\n",
    "tokenizer.fit_on_texts(sentences_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocabulary_size = len(word_index)\n",
    "LOG.info(\"Vocabulary created. Size: %s\", vocabulary_size)\n",
    "\n",
    "# Prepare the neural network inputs\n",
    "input_sentences_1 = tokenizer.texts_to_sequences(sentences_1)\n",
    "input_sentences_2 = tokenizer.texts_to_sequences(sentences_2)\n",
    "\n",
    "max_sentence_length = 0\n",
    "# The size of the input sequence is the size of the largest sequence of the input dataset\n",
    "for sentence_vec in [sentences_1, sentences_2]:\n",
    "    for sentence in sentence_vec:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if (sentence_length > max_sentence_length):\n",
    "            max_sentence_length = sentence_length\n",
    "\n",
    "x1 = pad_sequences(input_sentences_1, max_sequence_length)\n",
    "x2 = pad_sequences(input_sentences_2, max_sequence_length)\n",
    "# WARNING: STS LABEL RESCALING\n",
    "y = np.array(labels) / 5\n",
    "\n",
    "x1_train, x1_test, x2_train, x2_test, y_train, y_test = train_test_split(x1, x2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert x1_train.shape == x2_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare embedding matrix for word representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,689 : Loading embedding model from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,689 : Loading embedding model from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,689 : Loading embedding model from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,697 : loading projection weights from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,697 : loading projection weights from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:11:26,697 : loading projection weights from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:17,613 : loaded (3000000, 300) matrix from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:17,613 : loaded (3000000, 300) matrix from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:17,613 : loaded (3000000, 300) matrix from C:\\dev_env\\ml\\datasets\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:20,476 : Creating the embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:20,476 : Creating the embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:20,476 : Creating the embedding matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:33,201 : Embedding matrix as been created, removing embedding model from memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:33,201 : Embedding matrix as been created, removing embedding model from memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:15:33,201 : Embedding matrix as been created, removing embedding model from memory\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "LOG.info('Loading embedding model from %s', EMBEDDING_FILE)\n",
    "embedding_model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "LOG.info('Creating the embedding matrix')\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= vocabulary_size:\n",
    "        continue\n",
    "    if word in embedding_model.vocab:\n",
    "        embedding_vector = embedding_model.word_vec(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "#np.savetxt(\"embedding_matrix.txt\", embedding_matrix, delimiter=',')\n",
    "LOG.info('Embedding matrix as been created, removing embedding model from memory')\n",
    "del embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:16:24,104 : Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:16:24,104 : Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-02 12:16:24,104 : Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "LOG.info(\"Creating model...\")\n",
    "\n",
    "# ============= MODEL =====================\n",
    "# A entrada recebe os índices das palavras no vocabulário, para fazer o lookup na tabela de embeddings\n",
    "left_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "\n",
    "#Camada de embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "\n",
    "left_encoder = embedding_layer(left_input)\n",
    "right_encoder = embedding_layer(right_input)\n",
    "\n",
    "# LSTM\n",
    "LSTM_HIDDEN_LAYERS = 50\n",
    "base_lstm = LSTM(LSTM_HIDDEN_LAYERS)\n",
    "\n",
    "left_output = base_lstm(left_encoder)\n",
    "right_output = base_lstm(right_encoder)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                        output_shape=lambda x: (x[0][0], 1))\\\n",
    "    ([left_output, right_output])\n",
    "\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "gradient_clipping_norm = 1.25\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "malstm.compile(loss = 'mean_squared_error',\n",
    "               optimizer=optimizer,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "training_time = time()\n",
    "EPOCHS = 200\n",
    "malstm.fit([x1_train, x2_train], y_train,\n",
    "           epochs= EPOCHS,\n",
    "           batch_size=BATCH_SIZE,\n",
    "           validation_data=([x1_test, x2_test], y_test))\n",
    "\n",
    "print(\"\\nTraining time finished.\\n{} epochs in {}\".format(EPOCHS, datetime.timedelta(seconds=time()-training_time)))\n",
    "\n",
    "score, acc = malstm.evaluate([x1_test, x2_test], y_test, batch_size=BATCH_SIZE)\n",
    "print(\"\\nTest score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
